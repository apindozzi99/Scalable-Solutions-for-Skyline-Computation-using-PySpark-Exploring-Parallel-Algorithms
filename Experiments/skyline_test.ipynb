{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the cardinality\n",
    "\n",
    "This notebook allows to test the algorithms by changing the cardinality of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "from skyline import Skyline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_distribute = [\n",
    "    \"/home/pindozzi/skyline_code/skyline.py\",\n",
    "    \"/home/pindozzi/skyline_code/skyline_debug.py\",\n",
    "    \"/home/pindozzi/skyline_code/skylineV2.py\",\n",
    "    \"/home/pindozzi/skyline_code/skylineV3.py\",\n",
    "    \"/home/pindozzi/skyline_code/utils.py\",\n",
    "    \"/home/pindozzi/skyline_code/accumulator.py\",\n",
    "    \"/home/pindozzi/skyline_code/dataset.py\",\n",
    "    \"/home/pindozzi/skyline_code/grid_filtering.py\"\n",
    "]\n",
    "\n",
    "# Creazione della stringa con i percorsi dei file separati da virgole\n",
    "files_string = \",\".join(files_to_distribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "        master(\"spark://10.75.4.191:7077\").config(\"spark.files\", files_string).getOrCreate()\n",
    "print(\"spark session created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "skyline = Skyline()\n",
    "weights = [1.0, 1.0, 1.0, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataConfiguration = dataset.DataGenConfig(spreadPercentage=100, dataRange=[0,1])\n",
    "\n",
    "# uncomment the data distribution you prefer\n",
    "dataConfiguration.setAntiCorrelated() \n",
    "# dataConfiguration.setIndependent() \n",
    "# dataConfiguration.setCorrelated() \n",
    "dataConfiguration.numberOfDimensions = 4\n",
    "\n",
    "max_num_of_points = int(100*1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These rows of code are needed just to populate the x axis of the graphs\n",
    "points_array = [0, int(1e6), int(10*(1e6)), int(25*(1e6)), int(50*(1e6)), int(75*(1e6)), int(100*(1e6))]\n",
    "print(points_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain an array of different datasets (with increasing size)\n",
    "datasets_array = []\n",
    "for num in points_array[1:]:\n",
    "    dataConfiguration.setNumberOfData(num)\n",
    "    dataset = dataset.dataGenerator(dataConfiguration)\n",
    "    datasets_array.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the array of dataset in order to teset the algorithms with different dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_partitioning_sfs = [0]\n",
    "\n",
    "\n",
    "for data in datasets_array:\n",
    "    random = skyline.random_partitioning_sfs(spark, data, weights, 120)\n",
    "    random_partitioning_sfs.append(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_partitioning_sfs = [0]\n",
    "\n",
    "for data in datasets_array:\n",
    "    grid = skyline.parallel_grid_partitioning_sfs(spark, data, weights, 6)\n",
    "    grid_partitioning_sfs.append(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angular_partitioning_sfs = [0]\n",
    "\n",
    "for data in datasets_array:\n",
    "    angular = skyline.parallel_angled_partitioning_sfs(spark, data, weights, 6)\n",
    "    angular_partitioning_sfs.append(angular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_slice_partitioning_sfs = [0]\n",
    "\n",
    "for data in datasets_array:\n",
    "    one_slice = skyline.sliced_partitioning_sfs(spark, data, weights, 120)\n",
    "    one_slice_partitioning_sfs.append(one_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(points_array, one_slice_partitioning_sfs, \"r-\", label='sliced_partitioning_sfs')\n",
    "plt.plot(points_array, random_partitioning_sfs, \"b-\", label='random_partitioning_sfs')\n",
    "plt.plot(points_array, grid_partitioning_sfs, \"g-\", label='grid_partitioning_sfs')\n",
    "plt.plot(points_array, angular_partitioning_sfs, \"k-\", label='angular_partitioning_sfs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Parallel Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angular_with_dm = [0]\n",
    "\n",
    "for data in datasets_array:\n",
    "    time = skyline.angular_partition_with_sfs_representative_filtering_dominance_region(spark, data, weights, 6, 100)\n",
    "    angular_with_dm.append(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angular_with_angular = [0]\n",
    "\n",
    "for data in datasets_array:\n",
    "    time = skyline.angular_partition_with_sfs_representative_filtering_angular(spark, data, weights, 6, 100)\n",
    "    angular_with_angular.append(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_with_dm = [0]\n",
    "\n",
    "for data in datasets_array:\n",
    "    time = skyline.one_slice_with_sfs_representative_dominance_region(spark, data, weights, 120, 6, 100)\n",
    "    sliced_with_dm.append(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_with_angular = [0]\n",
    "\n",
    "for data in datasets_array:\n",
    "    time = skyline.one_slice_with_sfs_representative_filtering_angular(spark, data, weights, 120, 6, 100)\n",
    "    sliced_with_angular.append(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parallel = [0]\n",
    "\n",
    "for data in datasets_array:\n",
    "    time = skyline.AllParallel_sfs(spark, data, weights, numReps = 100)\n",
    "    sliced_with_angular.append(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(points_array, angular_with_dm, \"r-\", label='angular_partitioning_with_sfs_dominance_region')\n",
    "plt.plot(points_array, angular_with_angular, \"b-\", label='angular_partitioning_with_sfs_angular')\n",
    "plt.plot(points_array, sliced_with_dm, \"g-\", label='one_slice_with_sfs_dominance_region')\n",
    "#plt.plot(points_array, sliced_with_angular, \"k-\", label='one_slice_with_sfs_repr_angular')\n",
    "plt.plot(points_array, all_parallel, \"y-\", label='all_parallel_sfs')\n",
    "plt.title(\"Total Execution Time - Improved Algorithms - Anticorrelated Dataset\")\n",
    "plt.xlabel(\"Number of Data\")\n",
    "plt.ylabel(\"Time(s)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
